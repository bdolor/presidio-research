{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Flair models for person names, orgs and locations using the Presidio Evaluator framework\n",
    "\n",
    "Data = `test_February_28_2020`\n",
    "\n",
    "Note: *Make sure to run the notebook `Create datasets for Spacy training.ipynb` as the data generated is required for this notebook*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept 194 train samples after removal of non-tagged samples\n",
      "2020-03-22 17:53:22,532 Reading data from .\n",
      "2020-03-22 17:53:22,532 Train: flair_train.txt\n",
      "2020-03-22 17:53:22,532 Dev: flair_test.txt\n",
      "2020-03-22 17:53:22,532 Test: flair_val.txt\n",
      "Corpus: 194 train + 59 dev + 28 test sentences\n",
      "[b'<unk>', b'O', b'B-PERSON', b'I-PERSON', b'B-ORG', b'I-ORG', b'B-GPE', b'I-GPE', b'<START>', b'<STOP>']\n",
      "2020-03-22 17:53:23,432 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/glove.gensim.vectors.npy not found in cache, downloading to /tmp/tmpxkf2wtcs\n",
      "  0%|                              | 295936/160000128 [00:01<53:24, 49834.45B/s]^C\n",
      "Traceback (most recent call last):\n",
      "  File \"flair_train.py\", line 123, in <module>\n",
      "    trainer.train(corpus)\n",
      "  File \"flair_train.py\", line 75, in train\n",
      "    WordEmbeddings('glove'),\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/flair/embeddings.py\", line 224, in __init__\n",
      "    cached_path(f\"{old_base_path}glove.gensim.vectors.npy\", cache_dir=cache_dir)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/flair/file_utils.py\", line 88, in cached_path\n",
      "    return get_from_cache(url_or_filename, dataset_cache)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/flair/file_utils.py\", line 181, in get_from_cache\n",
      "    for chunk in req.iter_content(chunk_size=1024):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/requests/models.py\", line 750, in generate\n",
      "    for chunk in self.raw.stream(chunk_size, decode_content=True):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/urllib3/response.py\", line 496, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/urllib3/response.py\", line 444, in read\n",
      "    data = self._fp.read(amt)\n",
      "  File \"/opt/conda/lib/python3.7/http/client.py\", line 457, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"/opt/conda/lib/python3.7/http/client.py\", line 501, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"/opt/conda/lib/python3.7/socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/urllib3/contrib/pyopenssl.py\", line 297, in recv_into\n",
      "    return self.connection.recv_into(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/OpenSSL/SSL.py\", line 1839, in recv_into\n",
      "    result = _lib.SSL_read(self._ssl, buf, nbytes)\n",
      "KeyboardInterrupt\n",
      "  0%|                             | 364544/160000128 [00:01<08:44, 304486.41B/s]\n"
     ]
    }
   ],
   "source": [
    "! cd ../../models && python flair_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from presidio_evaluator.data_generator import read_synth_dataset\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select data for evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "synth_samples = read_synth_dataset(\"../../data/synth_dataset.json\")\n",
    "print(len(synth_samples))\n",
    "DATASET = synth_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "entity_counter = Counter()\n",
    "for sample in DATASET:\n",
    "    for tag in sample.tags:\n",
    "        entity_counter[tag]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "entity_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "DATASET[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#max length sentence\n",
    "max([len(sample.tokens) for sample in DATASET])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select models for evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "flair_ner = 'ner'\n",
    "flair_ner_fast = 'ner-fast'\n",
    "flair_ontonotes = 'ner-ontonotes-fast'\n",
    "flair_bert_embeddings = '../../models/presidio-ner/flair-bert-embeddings.pt'\n",
    "glove_flair_embeddings = '../../models/presidio-ner/flair-embeddings.pt'\n",
    "models = [flair_bert_embeddings, glove_flair_embeddings, flair_ner,flair_ner_fast,flair_ontonotes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from presidio_evaluator.flair_evaluator import FlairEvaluator\n",
    "\n",
    "for model in models:\n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"Evaluating model {}\".format(model))\n",
    "    flair_evaluator = FlairEvaluator(model_path=model)\n",
    "    evaluation_results = flair_evaluator.evaluate_all(DATASET)\n",
    "    scores = flair_evaluator.calculate_score(evaluation_results)\n",
    "    \n",
    "     \n",
    "    print(\"Confusion matrix:\")\n",
    "    print(scores.results)\n",
    "\n",
    "    print(\"Precision and recall\")\n",
    "    scores.print()\n",
    "    errors = scores.model_errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### False positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Most false positive tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "errors = scores.model_errors\n",
    "\n",
    "from presidio_evaluator import ModelEvaluator\n",
    "ModelEvaluator.most_common_fp_tokens(errors)#[model_error for model_error in errors if model_error.error_type =='FP']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "fps_df = ModelEvaluator.get_fps_dataframe(errors,entity=['PERSON'])\n",
    "fps_df[['full_text','token','prediction']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. False negative examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "ModelEvaluator.most_common_fn_tokens(errors,n=50, entity=['PERSON'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More FN analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "fns_df = ModelEvaluator.get_fns_dataframe(errors,entity=['PERSON'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fns_df[['full_text','token','annotation','prediction']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
